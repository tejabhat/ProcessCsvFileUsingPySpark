{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sree Ganeshaaya Namaha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read Events from a Big File using pySpark\n",
    "# 2. Remove the unwanted events using Spark DataFrame APIs\n",
    "# 3. Add new column to represent EPOCH time\n",
    "# 4. ==> Following doesn't work now, proabably, as I have used spark 2.3 with java 11, So use Spark itself to write the out file \n",
    "         # (Convert filtered list to a pandas dataframe => Spark dataframe.write.csv write operation writes segmented files. So, let us use pandas.)\n",
    "         # 5. Write the padas dataframe to the .csv file=> This will be a smaller file in comparison to the raw file and will be used for further processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"C:\\myprograms\\spark-2.3.0-bin-hadoop2.7\")\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "#udf, concat, desc, window\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "# Use tqdm to show progress of an pandas function we use\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create SparkSql Session\n",
    "use local node with 2 CPUs as master. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName(name=\"filter_file\").master(\"local[2]\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.csv(\"input/raw_events.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TIMESTAMP',\n",
       " 'HOST',\n",
       " 'FACILITY',\n",
       " 'SEVERITY',\n",
       " 'EPOLICY',\n",
       " 'EMESSAGE',\n",
       " 'SUPPRESSION_FLAG']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n",
    "#following gives error due to java 11 - df.select('HOST').count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SELECT only necessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['TIMESTAMP', 'HOST', 'EMESSAGE']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=df.select(\"TIMESTAMP\",\"HOST\",\"EMESSAGE\")\n",
    "df2.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#following code is supposed to work, but it doesn't work.\n",
    "#Let us give some time for pyspark to improve. \n",
    "#we use the other alternative to filter for now.\n",
    "#for more details - https://stackoverflow.com/questions/51710126/pyspark-regex-to-data-frame/54859546#54859546\n",
    "\"\"\"\n",
    "t=spark.createDataFrame([(1,\"hello a1\"),(2,\"hello b1\")])\n",
    "t.show()\n",
    "def t_filter(tline):\n",
    "    \n",
    "    print(tline)\n",
    "    if tline.contains(\"a1\"):\n",
    "        return False\n",
    "    return True\n",
    "t_udf=udf(t_filter,BooleanType())\n",
    "t2=t.filter(t_udf(t._2))\n",
    "t2.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_arr=[\"authen_faile\",\"sysmgr-\",\"recvd_pkt_inv_spi\",\"-power warning cleared\",\"-authen_success\",\n",
    "            \"sysmgr-standby-\",\"mgbl-exec-\",\"install-instmgr-\",\"-system_msg\",\"duplicate ip address\",\"mgbl-tty-\"\n",
    "            \"sysdb-sysdb-\",\"starservicevpnname\",\"ccmhistoryeventcommandsource\",\"\\%platform-\",\n",
    "            \" threshold:\",\" - interface utilization\",\"is active as part of bundle\",\n",
    "            \"are removed from suspended state\",\"counter overrun for one or more deltas\",\n",
    "            \"changed state to up\",\"security-tacacsd-%-server_down\",\"security-tacacsd-%-server_up\",\n",
    "            \"routing-ldp-%-rx_peer_dup_addr\",\"ip-tcp-%-badauth\",\"security-mpp-%-msg_info\",\n",
    "            \"routing-ldp-%-peer_dup_addrs\",\"bfd session to .%is up\",\"mgbl-ifstats-%-collector_timeout\",\"os-dumper-\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all rows where EMESSAGE column has one of the words given in filter_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+--------------------+\n",
      "|          TIMESTAMP|         HOST|            EMESSAGE|\n",
      "+-------------------+-------------+--------------------+\n",
      "|02/20/2015 11:02:26|   10.17.3.18|LC/0/11/CPU0:Feb ...|\n",
      "|02/20/2015 11:02:50|    10.17.3.4|LC/0/11/CPU0:Feb ...|\n",
      "|02/20/2015 15:02:09|   10.17.3.18|LC/0/11/CPU0:Feb ...|\n",
      "|02/20/2015 15:02:50|    10.17.3.4|LC/0/11/CPU0:Feb ...|\n",
      "|02/20/2015 19:02:09|   10.17.3.18|LC/0/11/CPU0:Feb ...|\n",
      "|09/29/2015 01:09:36|   10.17.3.82|LC/0/11/CPU0:Sep ...|\n",
      "|09/29/2015 01:09:28|   10.17.3.18|LC/0/11/CPU0:Sep ...|\n",
      "|09/29/2015 03:09:53|172.29.10.243|Sep 29 03:01:52.0...|\n",
      "|09/29/2015 03:09:09|   10.17.3.82|LC/0/11/CPU0:Sep ...|\n",
      "|09/29/2015 04:09:01|172.29.10.243|Sep 29 04:15:00.3...|\n",
      "|09/29/2015 06:09:48|172.29.10.243|Sep 29 06:08:48.7...|\n",
      "|09/29/2015 06:09:35|172.29.10.243|Sep 29 06:09:34.7...|\n",
      "|09/29/2015 06:09:29|172.29.10.243|Sep 29 06:37:28.6...|\n",
      "|09/29/2015 08:09:49| 10.17.131.27|LC/0/0/CPU0:Sep 2...|\n",
      "|09/29/2015 08:09:49| 10.17.131.27|RP/0/RP0/CPU0:Sep...|\n",
      "|09/29/2015 08:09:49| 10.17.131.27|RP/0/RP0/CPU0:Sep...|\n",
      "|09/29/2015 08:09:49| 10.17.131.27|RP/0/RP0/CPU0:Sep...|\n",
      "|09/29/2015 08:09:32|172.29.10.243|Sep 29 08:56:31.2...|\n",
      "|09/29/2015 09:09:30|   10.17.3.82|LC/0/11/CPU0:Sep ...|\n",
      "|09/29/2015 09:09:13|172.29.10.243|Sep 29 09:25:13.2...|\n",
      "+-------------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for phrase in filter_arr:\n",
    "    df2.createOrReplaceTempView(\"df_temp\")\n",
    "    df2 = spark.sql(\"select *, case when lower(EMESSAGE) like '%{}%' then 'TRUE' else 'FALSE' end filter_col from df_temp\".format(phrase))\n",
    "    df2=df2.filter(\"filter_col == FALSE\").select(\"TIMESTAMP\",\"HOST\",\"EMESSAGE\")\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# toPandas.collect().to_csv() is not working yet in pyspark.\n",
    "# Let us write using spark itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.repartition(1).write.csv(\"ouput/spark_filtered.csv\", sep=',')\n",
    "#df2.toPandas().collect().to_csv(\"faults/jaykumar/filtered_4.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following block is just for info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "# Use pandas_udf to define a Pandas UDF\n",
    "#@pandas_udf('double', PandasUDFType.SCALAR)\n",
    "# Input/output are both a pandas.Series of doubles\n",
    "\n",
    "#def pandas_plus_one(v):\n",
    "#    return v + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
